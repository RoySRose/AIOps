{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3a2H1ipSuZM5X+zRDsiIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoySRose/AIOps/blob/main/deeplog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Bf_JAXdf76"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def generate(name):\n",
        "    num_sessions = 0\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    with open('data/' + name, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            num_sessions += 1\n",
        "            line = tuple(map(lambda n: n - 1, map(int, line.strip().split())))\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "    print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "    print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "    dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_keys)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Hyperparameters\n",
        "    num_classes = 28\n",
        "    num_epochs = 300\n",
        "    batch_size = 2048\n",
        "    input_size = 1\n",
        "    model_dir = 'model'\n",
        "    log = 'Adam_batch_size={}_epoch={}'.format(str(batch_size), str(num_epochs))\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-num_layers', default=2, type=int)\n",
        "    parser.add_argument('-hidden_size', default=64, type=int)\n",
        "    parser.add_argument('-window_size', default=10, type=int)\n",
        "    args = parser.parse_args()\n",
        "    num_layers = args.num_layers\n",
        "    hidden_size = args.hidden_size\n",
        "    window_size = args.window_size\n",
        "\n",
        "    model = Model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "    seq_dataset = generate('hdfs_train')\n",
        "    dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    writer = SummaryWriter(log_dir='log/' + log)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    total_step = len(dataloader)\n",
        "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "        train_loss = 0\n",
        "        for step, (seq, label) in enumerate(dataloader):\n",
        "            # Forward pass\n",
        "            seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
        "            output = model(seq)\n",
        "            loss = criterion(output, label.to(device))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            writer.add_graph(model, seq)\n",
        "        print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "        writer.add_scalar('train_loss', train_loss / total_step, epoch + 1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "    if not os.path.isdir(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    torch.save(model.state_dict(), model_dir + '/' + log + '.pt')\n",
        "    writer.close()\n",
        "    print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}